# Software/DataHarvester_DDD/compose/docker-compose.yml

# To do; I need to break up the data harvester into a separate docker service. 
# I need to merge the genai-launchpad and data harvester services.
# I need to reach out to the department chair of education and ask if they'd be willing to work with me with to reach out to schools and help provide me with data.
# I need to utilize deepseek for inference.

services:
  shared:
    build:
      context: ..
      dockerfile: services/shared/Dockerfile.shared
    volumes:
      - shared_dist:/app/dist

  data_ingestion:
    build:
      context: ..
      dockerfile: services/data_ingestion/Dockerfile.DI
    container_name: "${PROJECT_NAME}_data_ingestion"
    environment:
      - MONGO_URI=mongodb://mongo:27017/
      - MONGO_DB=dataharvester
      - LOG_LEVEL=INFO
      - PYTHONUNBUFFERED=1
      - NLTK_DATA=/usr/local/share/nltk_data
      - SPACY_MODEL=en_core_web_sm
    volumes:
      - shared_dist:/shared_dist
      - ../data:/app/data
      - ../src:/app/src:ro
      - ../main.py:/app/main.py:ro
      - ../logs:/app/logs
      - models_data:/usr/local/lib/python3.12/site-packages/
      - nltk_data:/usr/local/share/nltk_data
      - spacy_data:/usr/local/lib/python3.12/site-packages/en_core_web_sm
      - shared_pkg:/app/shared:ro
    depends_on:
      - shared
      - mongo
    networks:
      - backend
      - database
    healthcheck:
      test: |
        python -c "
        import nltk, spacy, pymongo;
        nltk.data.find('tokenizers/punkt');
        spacy.load('en_core_web_sm');
        pymongo.MongoClient('mongodb://mongo:27017/').admin.command('ping')
        "
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  mongo:
    build:
      context: ../services/database
      dockerfile: Dockerfile.MongoDB
    container_name: "${PROJECT_NAME}_mongo"
    ports:
      - "${MONGO_PORT:-27019}:27017"
    volumes:
      - mongodb_data:/data/db
      - mongodb_config:/data/configdb
      - ../services/database/mongod.conf:/etc/mongod.conf
    command: [ "mongod", "--config", "/etc/mongod.conf" ]
    networks:
      - database
    healthcheck:
      test: [ "CMD", "mongosh", "--eval", "db.adminCommand('ping')" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  api:
    build:
      context: ..
      dockerfile: services/api/Dockerfile.api
    container_name: "${PROJECT_NAME}_api"
    depends_on:
      shared:
        condition: service_completed_successfully
      mongo:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports:
      - "8080:8080"
    restart: unless-stopped
    volumes:
      - shared_dist:/shared_dist
    networks:
      - frontend
      - backend
      - database
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      - PYTHONPATH=/app
      - MONGO_URI=mongodb://mongo:27017/

  caddy:
    container_name: "${PROJECT_NAME}_caddy"
    build:
      context: ../services/caddy
      dockerfile: Dockerfile.caddy
    env_file:
      - ./.env
    ports:
      - "80:80"
      - "127.0.0.1:2019:2019"
      - "443:443"
    restart: unless-stopped
    networks:
      - frontend
    volumes:
      - caddy_config:/config
      - caddy_data:/data
      - /var/log/caddy:/var/log/caddy
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      - api

  celery_worker:
    build:
      context: ..
      dockerfile: services/worker/Dockerfile.celery
    container_name: "${PROJECT_NAME}_celery_worker"
    depends_on:
      - redis
      - mongo
      - shared
    restart: unless-stopped
    volumes:
      - shared_dist:/shared_dist
    networks:
      - backend
      - database
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      - PYTHONPATH=/app
      - REDIS_URL=redis://redis:6379/0
      - MONGO_URI=mongodb://mongo:27017/

  redis:
    container_name: "${PROJECT_NAME}_redis"
    healthcheck:
      interval: 30s
      retries: 5
      test:
        - CMD
        - redis-cli
        - ping
      timeout: 10s
    image: redis:latest
    ports:
      - "6379:6379"
    restart: unless-stopped
    volumes:
      - redis_data:/data
    networks:
      - backend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  data_transformation:
    build:
      context: ..
      dockerfile: services/data_transformation/Dockerfile.DT
    container_name: "${PROJECT_NAME}_data_transformation"
    volumes:
      - shared_dist:/shared_dist
    environment:
      - PYTHONPATH=/app
      - MONGO_URI=mongodb://mongo:27017/
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
    depends_on:
      mongo:
        condition: service_healthy
      redis:
        condition: service_healthy
      shared:
        condition: service_healthy
    networks:
      - backend
      - database
    healthcheck:
      test: [ "CMD", "python", "-c", "import requests; requests.get('http://localhost:8000/health')" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

networks:
  frontend:
    name: dataharvester_frontend
    driver: bridge
  backend:
    name: dataharvester_backend
    driver: bridge
  database:
    name: dataharvester_database
    driver: bridge

volumes:
  mongodb_data:
  mongodb_config:
  models_data:
  nltk_data:
  spacy_data:
  caddy_config:
  caddy_data:
  redis_data:
  shared_pkg:
  shared_dist:
